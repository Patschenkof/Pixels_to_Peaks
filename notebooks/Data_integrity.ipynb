{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Howto\n",
    "\n",
    "- Different functions for different purposes. \n",
    "- First cell was to check, if tf.vectorized_map(mask_2_crop) is working, because that was were I got the first error\n",
    "- If files are found, use delete corrupted files to delete files from all directories\n",
    "- Another function is to check, if all files have the same dimension, so 256*256 for dem for example\n",
    "- check_sum_inner_outer checks, if the inner part of my mask (64*64) is equal to 4096. If not, this means, that functions inside of my model wont work\n",
    "\n",
    "## Best approach\n",
    "\n",
    "- Probably, just checking the sum of the inner-outer-mask is sufficient. Start with this, then delete files\n",
    "- Also check for division by zero at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# set up dataset\n",
    "version = 'V1.6'\n",
    "\n",
    "base_path = f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}/'\n",
    "\n",
    "dem_path = f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}/DEMs'\n",
    "inner_outer_path = f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}/Inner-Outer Mask'\n",
    "intersection_path = f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}/Intersection Mask'\n",
    "intersection_small_path = f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}/Intersection Mask Small'\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "corrupt_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if sum insider inner-outer is equal to 4096\n",
    "\n",
    "def check_sum_inner_outer(inner_outer_path, dim = (64,64)):\n",
    "\n",
    "    \"\"\"Checks if the sum of the inner-outer mask is equal to dim * dim\"\"\"\n",
    "    sum_value = dim[0] * dim[1]\n",
    "    corrupt_files = []\n",
    "    for filename in tqdm(os.listdir(inner_outer_path)):\n",
    "        filepath = os.path.join(inner_outer_path, filename)\n",
    "        data = np.load(filepath)\n",
    "        if np.sum(data) != sum_value:\n",
    "            corrupt_files.append(os.path.join(inner_outer_path, filename).encode())\n",
    "    return corrupt_files\n",
    "\n",
    "\n",
    "\n",
    "def check_min_max_values(folder_path, shape = (256,256)):\n",
    "\n",
    "    \"\"\"Checks if the min and max values of the DEMs are corrupt, to avoid division by zero errors\"\"\"\n",
    "\n",
    "    for file in tqdm(os.listdir(folder_path)):\n",
    "        filepath = os.path.join(folder_path, file)\n",
    "        data = cv2.imread(filepath, cv2.IMREAD_LOAD_GDAL)\n",
    "        data = cv2.resize(data, dsize=shape, interpolation=cv2.INTER_AREA)\n",
    "        if np.max(data) - np.min(data) == 0:\n",
    "            corrupt_files.append(os.path.join(folder_path, file).encode())\n",
    "    return corrupt_files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resize_dems(dem_path, shape = (256,256)):\n",
    "\n",
    "    \"\"\"Resizes the DEMs to the desired shape\"\"\"\n",
    "\n",
    "    for file in tqdm(os.listdir(dem_path)):\n",
    "        filepath = os.path.join(dem_path, file)\n",
    "        data = cv2.imread(filepath, cv2.IMREAD_LOAD_GDAL)\n",
    "        data = cv2.resize(data, dsize=shape, interpolation=cv2.INTER_AREA)\n",
    "        cv2.imwrite(filepath, data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def delete_corrupt_files(corrupt_files, dem_path, inner_outer_path, intersection_path ,intersection_small_path, base_path):\n",
    "\n",
    "    \"\"\"Deletes corrupt files from the dataset\n",
    "    Args:\n",
    "        corrupt_files (list): List of corrupt files\n",
    "        dem_path (str): Path to DEM files\n",
    "        inner_outer_path (str): Path to Inner-Outer Mask files\n",
    "        intersection_path (str): Path to Intersection Mask files\n",
    "        intersection_small_path (str): Path to Intersection Mask small files\"\"\"\n",
    "\n",
    "    for file in corrupt_files:\n",
    "        \n",
    "        if file.decode('UTF-8').endswith('.tif'):\n",
    "            dem_file = file.decode('UTF-8')\n",
    "        \n",
    "        if file.decode('UTF-8').endswith('.npy'):\n",
    "            dem_file = os.path.join(dem_path, os.path.basename(file.decode('UTF-8').replace('.npy', '.tif')))\n",
    "                                    \n",
    "        inner_outer_file = os.path.join(inner_outer_path, os.path.basename(file.decode('UTF-8')).replace('.tif', '.npy'))\n",
    "        intersection_file = os.path.join(intersection_path, os.path.basename(file.decode('UTF-8')).replace('.tif', '.npy'))\n",
    "        intersection_small_file = os.path.join(intersection_small_path, os.path.basename(file.decode('UTF-8')).replace('.tif', '.npy'))\n",
    "\n",
    "        os.remove(dem_file)\n",
    "        os.remove(inner_outer_file)\n",
    "        os.remove(intersection_file)\n",
    "        os.remove(intersection_small_file)\n",
    "\n",
    "        with open(os.path.join(base_path, 'corrupt_files.txt'), 'a') as f:\n",
    "            for item in corrupt_files:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "    print(f'Deleted {len(corrupt_files)} corrupt files')\n",
    "\n",
    "def norm_dems(dem_path):\n",
    "\n",
    "    for file in tqdm(os.listdir(dem_path)):\n",
    "        filepath = os.path.join(dem_path, file)\n",
    "        data = cv2.imread(filepath, cv2.IMREAD_LOAD_GDAL)\n",
    "        min_val = np.min(data)\n",
    "        max_val = np.max(data)\n",
    "\n",
    "        if max_val - min_val == 0:\n",
    "            raise ValueError(f'Min and max values are equal for file {file}')\n",
    "        \n",
    "        data = ((data - min_val) / (max_val - min_val)) * 2 -1 # normalize to [-1,1]\n",
    "        #cv2.imwrite(filepath, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize corrupt files list\n",
    "corrupt_files = []\n",
    "\n",
    "# check for corrupt files\n",
    "corrupt_sum = check_sum_inner_outer(inner_outer_path, dim = (64,64))\n",
    "corrupt_min_max = check_min_max_values(dem_path, shape = (256,256))\n",
    "\n",
    "\n",
    "# combine corrupt files and remove duplicates\n",
    "corrupt_files = corrupt_sum + corrupt_min_max\n",
    "corrupt_files = list(dict.fromkeys(corrupt_files))\n",
    "\n",
    "# delete corrupt files\n",
    "delete_corrupt_files(corrupt_files, dem_path, inner_outer_path, intersection_path, intersection_small_path, base_path)\n",
    "\n",
    "# resize and normalize DEMs\n",
    "resize_dems(dem_path, shape = (256,256))\n",
    "norm_dems(dem_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further is probably not necessary. The cells above should ensure integrity of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates over the whole dataset and checks if the cropping algorithm works correctly\n",
    "\n",
    "corrupt_files = []\n",
    "\n",
    "logging.basicConfig(filename='/home/robin/Nextcloud_sn/Masterarbeit/DataSet/V1.3/integrity_check' + timestr + '.log', level=logging.ERROR)\n",
    "\n",
    "# Define file patterns\n",
    "dem_pattern = os.path.join(dem_path, \"*.tif\")  # Assuming your DEM files are in TIFF format\n",
    "inner_outer_pattern = os.path.join(inner_outer_path, \"*.npy\")\n",
    "intersection_small_pattern = os.path.join(intersection_small_path, \"*.npy\")\n",
    "\n",
    "# Create datasets\n",
    "dem_dataset = tf.data.Dataset.list_files(dem_pattern, shuffle=False)\n",
    "inner_outer_dataset = tf.data.Dataset.list_files(inner_outer_pattern, shuffle=False)\n",
    "intersection_small_dataset = tf.data.Dataset.list_files(intersection_small_pattern, shuffle=False)\n",
    "\n",
    "# Convert to lists if needed\n",
    "dem_list = list(dem_dataset.as_numpy_iterator())\n",
    "inner_outer_list = list(inner_outer_dataset.as_numpy_iterator())\n",
    "intersection_small_list = list(intersection_small_dataset.as_numpy_iterator())\n",
    "\n",
    "\n",
    "# Sort the lists, so that files align correctly\n",
    "dem_list.sort()\n",
    "inner_outer_list.sort()\n",
    "intersection_small_list.sort()\n",
    "\n",
    "\n",
    "print('Starting to iterate over whole dataset')\n",
    "for i in tqdm(range(len(dem_list))):\n",
    "    f_dem = dem_list[i]\n",
    "    f_inner_outer = inner_outer_list[i]\n",
    "    f_intersection_small = intersection_small_list[i]\n",
    "\n",
    "    # Load the DEM\n",
    "    data_dem = cv2.imread(f_dem.decode('Utf-8'), cv2.IMREAD_LOAD_GDAL)\n",
    "    data_dem = cv2.resize(data_dem, dsize=(256,256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    image = tf.convert_to_tensor(data_dem)\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "\n",
    "    # Load the Inner-Outer Mask\n",
    "    data_inner_outer = np.load(f_inner_outer)\n",
    "    inner_outer = tf.convert_to_tensor(data_inner_outer)\n",
    "    inner_outer = tf.expand_dims(inner_outer, axis=-1)\n",
    "    mask_inner_outer = tf.image.resize(inner_outer, (256,256), method='nearest')\n",
    "\n",
    "    # Load intersection_mask_small\n",
    "    data_intersection_small = np.load(f_intersection_small)\n",
    "    intersection_small = tf.convert_to_tensor(data_intersection_small)\n",
    "    intersection_small = tf.expand_dims(intersection_small, axis=-1)\n",
    "    mask_intersection_small = tf.image.resize(intersection_small, (64,64), method='nearest')\n",
    "\n",
    "    # Find indices where mask_inner_outer is 1\n",
    "    if tf.reduce_any(mask_inner_outer == 1):\n",
    "\n",
    "        indices = tf.where(mask_inner_outer == 1)\n",
    "        top_left = tf.reduce_min(indices, axis=0)\n",
    "\n",
    "        x1 = tf.cast(top_left[0], tf.int32)\n",
    "        y1 = tf.cast(top_left[1], tf.int32)\n",
    "\n",
    "        shapex = tf.constant(64, tf.int32)\n",
    "        shapey = tf.constant(64, tf.int32)\n",
    "\n",
    "        try:\n",
    "            crop = tf.image.crop_to_bounding_box(image, x1, y1, shapex, shapey)\n",
    "        except (IOError, ValueError, tf.errors.OpError) as e:\n",
    "            logging.error(f\"Error processing file {f_dem}: {e}\")\n",
    "            corrupt_files.append(f_dem)\n",
    "            continue\n",
    "\n",
    "        # test if crop has right shape\n",
    "        if crop.shape != (64,64,1):\n",
    "            raise ValueError(f'Error: {f_dem}')\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f'No intersection found for {f_inner_outer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for file size mismatch  \n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#dem_path = '/mnt/HDD/Masterarbeit_local/DEMs'\n",
    "#inner_outer_path = '/mnt/HDD/Masterarbeit_local/Inner-Outer Mask'\n",
    "#intersection_path = '//mnt/HDD/Masterarbeit_local/Intersection Mask'\n",
    "#intersection_small_path = '/mnt/HDD/Masterarbeit_local/Intersection Mask small'\n",
    "\n",
    "def check_image_dimension(folder_path, desired_shape = (128,128), extensions=(\".jpg\", \".jpeg\", \".png\", \".tif\", \".npy\")):\n",
    "\n",
    "    \"\"\"Checks if the images in the dataset have the desired shape\"\"\"\n",
    "\n",
    "    mismatched_files = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "\n",
    "        if not filename.lower().endswith(extensions):\n",
    "            raise ValueError(f'File {filename} is not in a desired file format')\n",
    "        \n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "        if filename.endswith('.npy'):\n",
    "            try:\n",
    "                data = np.load(filepath)\n",
    "                if data.shape != desired_shape:\n",
    "                    mismatched_files.append(filename)\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "        elif filename.endswith('.tif'):\n",
    "            try:\n",
    "                data = cv2.imread(filepath, cv2.IMREAD_LOAD_GDAL)\n",
    "                if data.shape[:2] != desired_shape:\n",
    "                    mismatched_files.append(filename)\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "        else:\n",
    "            try:\n",
    "                data = cv2.imread(filepath)\n",
    "                if data.shape[:2] != desired_shape:\n",
    "                    mismatched_files.append(filename)\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "    \n",
    "    return mismatched_files\n",
    "\n",
    "mismatched_dem = check_image_dimension(dem_path, desired_shape=(256,256))\n",
    "mismatched_inner_outer = check_image_dimension(inner_outer_path, desired_shape=(256,256))\n",
    "mismatched_intersection = check_image_dimension(intersection_path, desired_shape=(256,256))\n",
    "mismatched_intersection_small = check_image_dimension(intersection_small_path, desired_shape=(64,64))\n",
    "\n",
    "print(len(mismatched_dem))\n",
    "print(len(mismatched_inner_outer))\n",
    "print(len(mismatched_intersection))\n",
    "print(len(mismatched_intersection_small))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also remember to look out for dtype of numpy masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably not necessary\n",
    "# corrupt_encode = [file.encode() for file in corrupt_files]\n",
    "# corrupt_files = [file.decode('UTF-8') for file in corrupt_encode]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
