{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook used to primarily generate training dataset, using GLIMS, Google Earth Engine and geemap. Further down below are also the scripts\n",
    "### used to generate evaluation data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# Only used to preprocess the GLIMS dataset. Only needs to be executed once!      #\n",
    "#                                                                                 #\n",
    "#                                                                                 # \n",
    "###################################################################################\n",
    "\n",
    "\n",
    "# Import needed libraries\n",
    "import sys\n",
    "import dask_geopandas as dg\n",
    "import pandas as pd\n",
    "sys.path.append('..')\n",
    "from scripts.data_processing_utils import DaskUtils, GISProcessor\n",
    "\n",
    "# initialize Dask for filtering of the dataset\n",
    "\n",
    "GISProcessor = GISProcessor()\n",
    "\n",
    "# Get target columns\n",
    "target_columns = ['line_type', 'glac_id', 'src_date', 'glac_stat', 'geometry']\n",
    "\n",
    "# Read the dataset\n",
    "fp = \"/home/robin/Nextcloud_sn/QGIS/raw/glims_download_19805/glims_polygons.shp\"\n",
    "\n",
    "ddf = dg.read_file(fp, npartitions=6)\n",
    "ddf = ddf[target_columns]\n",
    "\n",
    "# Filter the dataset\n",
    "df_bound_gone = ddf[(ddf[\"glac_stat\"] == \"gone\") & (ddf['line_type'] == 'glac_bound')].compute() \n",
    "df_bound_exist = ddf[(ddf[\"glac_stat\"] == \"exists\") & (ddf['line_type'] == 'glac_bound')].compute()\n",
    "\n",
    "\n",
    "# Drop the line_type column\n",
    "df_bound_gone.drop(columns=['line_type'], inplace=True)\n",
    "df_bound_exist.drop(columns=['line_type'], inplace=True)\n",
    "\n",
    "# Convert the src_date column to datetime\n",
    "df_bound_gone['src_date'] = pd.to_datetime(df_bound_gone['src_date'])\n",
    "df_bound_exist['src_date'] = pd.to_datetime(df_bound_exist['src_date'])\n",
    "\n",
    "# Dissolve the dataframes\n",
    "df_bound_exist_dissolved = GISProcessor.dissolve_glaciers(df_bound_exist)\n",
    "df_bound_gone_dissolved = GISProcessor.dissolve_glaciers(df_bound_gone)\n",
    "\n",
    "# Drop the glac_stat column\n",
    "df_bound_exist_dissolved.drop(columns=['glac_stat'], inplace=True)\n",
    "df_bound_gone_dissolved.drop(columns=['glac_stat'], inplace=True)\n",
    "\n",
    "# Make a residual dataset\n",
    "df_residual = GISProcessor.make_residuals(df_bound_exist_dissolved, min_change=5, max_change=40)\n",
    "\n",
    "# Remove MultiPolygon geometries\n",
    "df_residual['geometry'] = df_residual['geometry'].apply(GISProcessor.keep_largest_multipolygon)\n",
    "\n",
    "\n",
    "# Save the filtered dataset\n",
    "fp_out = \"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/test_refactor_code/03.07.2024\"\n",
    "\n",
    "#df_exists_bound.to_file(fp_out + \"/exists_bound.shp\")\n",
    "df_bound_gone_dissolved.to_file(fp_out + \"/gone_bound.shp\")\n",
    "df_residual.to_file(fp_out + \"/residuals.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.data_processing_utils import DatasetMaker\n",
    "import os\n",
    "import geemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ee\n",
    "import logging\n",
    "import time\n",
    "\n",
    "\n",
    "# Set paths and dataset version\n",
    "version = \"V1.6\"\n",
    "\n",
    "# Create the directories\n",
    "base_data_path = f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}'\n",
    "sub_paths = ['DEMs', 'Inner-Outer Mask', 'Intersection Mask', 'Intersection Mask Small']\n",
    "\n",
    "for sub_path in sub_paths:\n",
    "    full_path = os.path.join(base_data_path, sub_path)\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "# Set the paths\n",
    "file_path_dem = f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}/DEMs'\n",
    "path_inner_outer = f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}/Inner-Outer Mask'\n",
    "path_intersection = f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}/Intersection Mask'\n",
    "path_intersection_small = f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}/Intersection Mask Small'\n",
    "\n",
    "# Set the logging level\n",
    "logging.basicConfig(filename=f'/home/robin/Nextcloud_sn/Masterarbeit/DataSet/{version}/logging.txt',level=logging.ERROR)\n",
    "\n",
    "# Initialize the Earth Engine\n",
    "geemap.ee_initialize()\n",
    "\n",
    "# Initialize the DatasetMaker\n",
    "ds_maker = DatasetMaker()\n",
    "\n",
    "# Read the saved datasets\n",
    "df_residuals = gpd.read_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/test_refactor_code/03.07.2024/residuals.shp\")\n",
    "df_gone = gpd.read_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/test_refactor_code/03.07.2024/gone_bound.shp\")\n",
    "\n",
    "# Concatenate the datasets\n",
    "\n",
    "df = pd.concat([df_residuals[['glac_id', 'geometry']], df_gone[['glac_id', 'geometry']]])\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "df = ds_maker.filter_glaciers_by_country(df, ['Armenia', 'Azerbaijan'])\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "# Mapping of masked pixels\n",
    "value_mapping = {\n",
    "    'Void (no data)': 0,\n",
    "    'Edited (except filled pixel)': 1,\n",
    "    'Not edited / not filled': 2,\n",
    "    'ASTER': 3,\n",
    "    'SRTM90': 4,\n",
    "    'SRTM30': 5,\n",
    "    'GMTED2010': 6,\n",
    "    'SRTM30plus': 7,\n",
    "    'TerraSAR-X Radargrammetric DEM': 8,\n",
    "    'AW3D30': 9,\n",
    "    'Norway DEM': 100,\n",
    "    'DSM05 Spain': 101\n",
    "}\n",
    "\n",
    "df_flm = pd.DataFrame(columns=['glac_id', 'Void (no data)', 'Edited (except filled pixel)', 'Not edited / not filled', 'ASTER', 'SRTM90', 'SRTM30', 'GMTED2010',\n",
    "                                  'SRTM30plus', 'TerraSAR-X Radargrammetric DEM', 'AW3D30', 'Norway DEM', 'DSM05 Spain', 'overall_sum'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supresses geemap output. However, it is hard to know, how far the download has progressed!\n",
    "# To supress, just switch this function with the ee_export_image function in the cell below or vice versa.\n",
    "# TQDM would be a better solution, however, it oftentimes halted the execution of the script.\n",
    "import contextlib\n",
    "\n",
    "def ee_export_image(*args, **kwargs):\n",
    "    with open(os.path.join(base_data_path, 'logging_gee.txt'), 'a') as f:\n",
    "        with contextlib.redirect_stdout(f):\n",
    "            geemap.ee_export_image(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = df['glac_id'].unique()\n",
    "failed_ids = []\n",
    "large_ids = []\n",
    "small_ids = []\n",
    "scale = 30\n",
    "large_dim = 256\n",
    "small_dim = 64\n",
    "\n",
    "\n",
    "\n",
    "for glac_id in unique_ids:\n",
    "\n",
    "    \n",
    "    # Get the glacier dataframe\n",
    "    \n",
    "    df_glacid = df[df['glac_id'] == glac_id]\n",
    "    \n",
    "    # Same Filename for all tiff\n",
    "    file_name = str(glac_id) + '.tif'\n",
    "    # same filename for all masks\n",
    "    file_name_mask = str(glac_id) + '.npy'\n",
    "\n",
    "    # Values for the while loop, checking correct inner-outer mask generation\n",
    "    tries_inner_outer = 0\n",
    "    max_tries_inner_outer = 1000\n",
    "    truth_value = True\n",
    "\n",
    "    try:\n",
    "        if ds_maker.glacier_area_check(scale, large_dim, df_glacid): # Area of glacier bounds is larger than area of mask\n",
    "            \n",
    "            # Get random bbox intersecting the glacier\n",
    "            gdf_bbox_rand, gdf_intersection = ds_maker.get_random_bbox_intersect(df_glacid, buffer = 5000, min_coverage = 0.05, \n",
    "                                                                                 max_coverage = 0.9,img_res = large_dim, max_tries= 1000)\n",
    "\n",
    "            if gdf_bbox_rand is None or gdf_intersection is None:\n",
    "                failed_ids.append(glac_id)\n",
    "                continue\n",
    "\n",
    "            # Get random bbox again but in small scale (64 x 64)\n",
    "            gdf_bbox_rand_small, gdf_intersection_small = ds_maker.get_random_bbox_intersect(gdf_intersection, spatial_resolution = scale, \n",
    "                                                                                             img_res = small_dim, buffer = 0, min_coverage = 0.05,\n",
    "                                                                                             max_tries= 1000)\n",
    "\n",
    "            if gdf_bbox_rand_small is None or gdf_intersection_small is None:\n",
    "                failed_ids.append(glac_id)\n",
    "                continue\n",
    "                \n",
    "\n",
    "            # Download and Save DEM\n",
    "            path_abs = os.path.join(file_path_dem, file_name)        \n",
    "            sq_geom = geemap.geopandas_to_ee(gdf_bbox_rand)\n",
    "            filtered_band = ee.ImageCollection('COPERNICUS/DEM/GLO30').select('DEM').filterBounds(sq_geom).mean()\n",
    "            ee_export_image(filtered_band, filename=path_abs, scale=scale, region=sq_geom.geometry().bounds(), file_per_band=False, crs = 'EPSG:3857')\n",
    "\n",
    "            # Get FLM table:\n",
    "            flm_table = ee.ImageCollection('COPERNICUS/DEM/GLO30').select('FLM').filterBounds(sq_geom).mean()\n",
    "            flm_array = geemap.ee_to_numpy(flm_table, region=sq_geom.geometry().bounds(), scale=scale)\n",
    "\n",
    "            # Set glac_id for which values are stored\n",
    "            df_flm.loc[len(df_flm.index), 'glac_id'] = glac_id\n",
    "\n",
    "            # Get unique values and their counts\n",
    "            unique_labels, count = np.unique(flm_array, return_counts=True)\n",
    "\n",
    "            # Store values in the dataframe\n",
    "            for key in value_mapping.keys():\n",
    "                df_flm.loc[df_flm.index[df_flm['glac_id'] == glac_id][0], key] = np.sum(count[unique_labels == value_mapping[key]])\n",
    "                df_flm.loc[df_flm.index[df_flm['glac_id'] == glac_id][0], 'overall_sum'] = len(flm_array.flatten())\n",
    "\n",
    "            # Generate masks\n",
    "            inner_outer_mask, intersection_mask, intersection_mask_small = ds_maker.bbox_2_mask_rasterized(gdf_bbox_rand, gdf_bbox_rand_small, gdf_intersection, gdf_intersection_small,\n",
    "                                                                                                           properties = {\n",
    "                                                                                                            'outer_dim' : (large_dim, large_dim),\n",
    "                                                                                                            'inner_dim' : (small_dim, small_dim),\n",
    "                                                                                                        })\n",
    "            # inner_outer_mask, intersection_mask, intersection_mask_small = Image.fromarray(inner_outer_mask), Image.fromarray(intersection_mask), Image.fromarray(intersection_mask_small)\n",
    "\n",
    "            # Save masks\n",
    "            np.save(os.path.join(path_inner_outer, file_name_mask), inner_outer_mask)\n",
    "            np.save(os.path.join(path_intersection, file_name_mask), intersection_mask)\n",
    "            np.save(os.path.join(path_intersection_small, file_name_mask), intersection_mask_small)\n",
    "\n",
    "            large_ids.append(glac_id)\n",
    "        \n",
    "\n",
    "        elif not ds_maker.glacier_area_check(scale, large_dim, df_glacid): # Area of glacier bounds is smaller than area of mask\n",
    "\n",
    "            gdf_bbox_rand, gdf_intersection = ds_maker.get_randomize_center_bbox(df_glacid, spatial_resolution=scale, img_res=large_dim)\n",
    "            \n",
    "            if ds_maker.glacier_area_check(scale, small_dim, gdf_intersection): # Area of glacier bounds is larger than area of mask\n",
    "                gdf_bbox_rand_small, gdf_intersection_small = ds_maker.get_random_bbox_intersect(gdf_intersection, spatial_resolution = scale,\n",
    "                                                                                                 img_res = small_dim, buffer = 0, min_coverage = 0.05,\n",
    "                                                                                                 max_tries = 1000)\n",
    "\n",
    "                # Chech if function workes\n",
    "                if gdf_bbox_rand_small is None or gdf_intersection_small is None:\n",
    "                    failed_ids.append(glac_id)\n",
    "                    continue\n",
    "                else:\n",
    "                    # Generate masks\n",
    "                    inner_outer_mask, intersection_mask, intersection_mask_small = ds_maker.bbox_2_mask_rasterized(gdf_bbox_rand, gdf_bbox_rand_small, gdf_intersection, gdf_intersection_small,\n",
    "                                                                                                                   properties = {\n",
    "                                                                                                            'outer_dim' : (large_dim, large_dim),\n",
    "                                                                                                            'inner_dim' : (small_dim, small_dim),\n",
    "                                                                                                        })\n",
    "\n",
    "            else: # Area of glacier bounds is smaller than area of mask\n",
    "\n",
    "                while tries_inner_outer < max_tries_inner_outer:\n",
    "                    '''\n",
    "                    Check if the sum of the inner and outer mask is equal to a specific dimension like 64*64. If not, try again.\n",
    "                    If a correct result is archived, the loop will break. If not, the loop will break after 1000 tries. If it breaks after a 1000 tries, the glacier id will be added to the failed_ids list\n",
    "                    and the entry will be skipped\n",
    "                    '''\n",
    "\n",
    "                    tries_inner_outer += 1\n",
    "\n",
    "                    gdf_bbox_rand_small, gdf_intersection_small = ds_maker.get_randomize_center_bbox(gdf_intersection, spatial_resolution=scale, img_res=small_dim)\n",
    "\n",
    "                    # Generate masks. Intersection mask is not used. We need another function here\n",
    "                    inner_outer_mask, intersection_mask, intersection_mask_small = ds_maker.bbox_2_mask_rasterized(gdf_bbox_rand, gdf_bbox_rand_small, gdf_intersection, gdf_intersection_small,\n",
    "                                                                                                                   properties = {\n",
    "                                                                                                            'outer_dim' : (large_dim, large_dim),\n",
    "                                                                                                            'inner_dim' : (small_dim, small_dim),\n",
    "                                                                                                        })                    \n",
    "                    if ds_maker.check_sum_inner_outer(inner_outer_mask, dim = (small_dim, small_dim)):\n",
    "                        truth_value = False\n",
    "                        break\n",
    "\n",
    "                if truth_value:\n",
    "                    failed_ids.append(glac_id)\n",
    "                    continue\n",
    "\n",
    "            path_abs = os.path.join(file_path_dem, file_name)        \n",
    "            sq_geom = geemap.geopandas_to_ee(gdf_bbox_rand)\n",
    "            filtered_band = ee.ImageCollection('COPERNICUS/DEM/GLO30').select('DEM').filterBounds(sq_geom).mean()\n",
    "            ee_export_image(filtered_band, filename=path_abs, scale=scale, region=sq_geom.geometry().bounds(), file_per_band=False, crs = 'EPSG:3857')\n",
    "\n",
    "            # Get FLM table:\n",
    "            flm_table = ee.ImageCollection('COPERNICUS/DEM/GLO30').select('FLM').filterBounds(sq_geom).mean()\n",
    "            flm_array = geemap.ee_to_numpy(flm_table, region=sq_geom.geometry().bounds(), scale=scale)\n",
    "\n",
    "            # Set glac_id for which values are stored\n",
    "            df_flm.loc[len(df_flm.index), 'glac_id'] = glac_id\n",
    "\n",
    "            # Get unique values and their counts\n",
    "            unique_labels, count = np.unique(flm_array, return_counts=True)\n",
    "\n",
    "            # Store values in the dataframe\n",
    "            for key in value_mapping.keys():\n",
    "                df_flm.loc[df_flm.index[df_flm['glac_id'] == glac_id][0], key] = np.sum(count[unique_labels == value_mapping[key]])\n",
    "                df_flm.loc[df_flm.index[df_flm['glac_id'] == glac_id][0], 'overall_sum'] = len(flm_array.flatten())\n",
    "\n",
    "            # Transform masks in appropriate format        \n",
    "            #inner_outer_mask, intersection_mask, intersection_mask_small = Image.fromarray(inner_outer_mask), Image.fromarray(intersection_mask), Image.fromarray(intersection_mask_small)\n",
    "\n",
    "            # Save masks\n",
    "            np.save(os.path.join(path_inner_outer, file_name_mask), inner_outer_mask)\n",
    "            np.save(os.path.join(path_intersection, file_name_mask), intersection_mask)\n",
    "            np.save(os.path.join(path_intersection_small, file_name_mask), intersection_mask_small)\n",
    "\n",
    "            small_ids.append(glac_id)  \n",
    "\n",
    "\n",
    "        else:\n",
    "            print('Found glacier with equal bounds in id: ', glac_id)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e} in glacier {glac_id}\")\n",
    "        print(f\"Error: {e} in glacier {glac_id}\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Check if all files are saved correctly\n",
    "    folder_paths = [file_path_dem, path_inner_outer, path_intersection, path_intersection_small]\n",
    "    file_counts = []\n",
    "\n",
    "    for folder in folder_paths:\n",
    "        file_counts.append(len([name for name in os.listdir(folder) if os.path.isfile(os.path.join(folder, name))]))\n",
    "\n",
    "    if len(set(file_counts)) != 1:  # Check if all counts are the same\n",
    "        raise ValueError(f\"Error: Mismatched file counts in folders after processing glacier {glac_id}: {file_counts}\")\n",
    "    \n",
    "\n",
    "    with open(os.path.join(base_data_path, 'failed_ids.txt'), 'w') as f:\n",
    "        for item in failed_ids:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    with open(os.path.join(base_data_path, 'large_ids.txt'), 'w') as f:\n",
    "        for item in large_ids:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    with open(os.path.join(base_data_path, 'small_ids.txt'), 'w') as f:\n",
    "        for item in small_ids:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "\n",
    "df_flm.to_csv(os.path.join(base_data_path, 'flm_values.csv'), mode='w', index=False)\n",
    "\n",
    "print('All files saved correctly\\n Finished processing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down below are cells used to evaluate model performanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Dataset for Evaluation Purposes #########################################\n",
    "\n",
    "###################################################### Create gla_thi_da shapefile ######################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Read the dataset\n",
    "fp = \"/home/robin/Nextcloud_sn/QGIS/raw/GlaThiDa/glathida-3.1.0/glathida-3.1.0/data/TTT.csv\"\n",
    "\n",
    "gla_thi_da = pd.read_csv(fp, sep=\",\")\n",
    "gla_thi_da['SURVEY_DATE'] = gla_thi_da['SURVEY_DATE'].astype(str)\n",
    "gla_thi_da['YEAR'] = gla_thi_da['SURVEY_DATE'].str[:4]\n",
    "#gla_thi_da['YEAR'] = pd.to_datetime(gla_thi_da['YEAR'], format='%Y')\n",
    "\n",
    "gla_thi_da.rename(columns={'POINT_LAT': 'lat', 'POINT_LON': 'lon', 'GLACIER_NAME': 'glac_name', 'YEAR': 'year'}, inplace=True)\n",
    "gla_thi_da['glac_name'] = gla_thi_da['glac_name'].str.lower()\n",
    "gla_thi_da['glac_name'] = gla_thi_da['glac_name'].str.replace(' ', '_')\n",
    "\n",
    "gla_thi_da = gpd.GeoDataFrame(gla_thi_da, geometry=gpd.points_from_xy(gla_thi_da.lon, gla_thi_da.lat), crs='EPSG:4326')\n",
    "gla_thi_da.rename_geometry('point_geom', inplace=True)\n",
    "\n",
    "cols_to_drop = ['GlaThiDa_ID', 'POLITICAL_UNIT', 'SURVEY_DATE', 'PROFILE_ID', 'POINT_ID', 'lat', 'lon']\n",
    "gla_thi_da.drop(columns= cols_to_drop, inplace=True)\n",
    "gla_thi_da.reset_index(drop=True, inplace=True)\n",
    "\n",
    "gla_thi_da.to_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/eval/TTT.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### Create glims_polygons shapefile ######################################################\n",
    "import dask_geopandas as dg\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.data_processing_utils import GISProcessor\n",
    "gis_processor = GISProcessor()\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "fp = \"/home/robin/Nextcloud_sn/QGIS/raw/glims_download_19805/glims_polygons.shp\"\n",
    "# Get target columns\n",
    "target_columns = ['line_type', 'glac_id', 'src_date', 'glac_stat', 'geometry', 'glac_name']\n",
    "\n",
    "ddf = dg.read_file(fp, npartitions=6)\n",
    "ddf = ddf[target_columns]\n",
    "\n",
    "ddf = ddf[(ddf['glac_stat'] == 'exists') & (ddf['line_type'] == 'glac_bound')].compute()\n",
    "\n",
    "ddf.drop(columns=['line_type'], inplace=True)\n",
    "ddf['src_date'] = pd.to_datetime(ddf['src_date'])\n",
    "ddf = gis_processor.dissolve_glaciers(ddf)\n",
    "ddf.drop(columns=['glac_stat'], inplace=True)\n",
    "\n",
    "ddf.rename_geometry('outline_glaciers', inplace=True)\n",
    "ddf['glac_name'] = ddf['glac_name'].str.lower()\n",
    "ddf['glac_name'] = ddf['glac_name'].str.replace(' ', '_')\n",
    "ddf['year'] = ddf['year'].astype(str)\n",
    "\n",
    "ddf.rename_geometry('polygon_geom', inplace=True)\n",
    "ddf.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "ddf.to_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/eval/glims_polygons.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### Create gla_rho shapefile ######################################################\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import dask_geopandas as dg\n",
    "\n",
    "# Read the dataset\n",
    "gla_rho = gpd.read_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/eval/gla_rho.shp\")\n",
    "glims_rho = gpd.read_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/eval/glims_rho.shp\")\n",
    "\n",
    "# Filter the data\n",
    "gla_rho_2008 = gla_rho[gla_rho['year'] == '2008']\n",
    "glims_rho_2008 = glims_rho[glims_rho['year'] == '2008']\n",
    "\n",
    "# Reproject the data\n",
    "gla_rho_2008 = gla_rho_2008.to_crs('EPSG:3857')\n",
    "glims_rho_2008 = glims_rho_2008.to_crs('EPSG:3857')\n",
    "\n",
    "# Spatial join\n",
    "s_join = gpd.sjoin(gla_rho_2008, glims_rho_2008[['geometry']], how='inner', op='within')\n",
    "s_join.head()\n",
    "\n",
    "# Merge the data\n",
    "merged = s_join.merge(glims_rho_2008[['geometry', 'glac_name']], how='left', on='glac_name')\n",
    "merged.rename(columns = {'geometry_x': 'gla_rho_geom', 'geometry_y': 'glims_rho_geom'}, inplace=True)\n",
    "merged.drop(columns=['index_right'], inplace=True)\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "merged_gdf = gpd.GeoDataFrame(merged, crs='EPSG:3857', geometry=merged['gla_rho_geom'])\n",
    "merged_gdf.drop(columns=['gla_rho_geom'], inplace=True)\n",
    "merged_gdf.rename(columns={'geometry': 'gla_rho_geom'}, inplace=True)\n",
    "\n",
    "# Set the geometry\n",
    "merged_gdf.set_geometry('gla_rho_geom', inplace=True)\n",
    "\n",
    "# Set glims to wkt. GeoDataFrame does not support more than one geometry column!\n",
    "merged_gdf['glims_wkt'] = merged_gdf['glims_rho_geom'].to_wkt()\n",
    "merged_gdf.drop(columns=['glims_rho_geom', 'glims_wkt'], inplace=True)\n",
    "merged_gdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the data\n",
    "merged_gdf.to_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/eval/eval_rhone.gpkg\", driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### load rhone glacier ######################################################\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geemap\n",
    "import ee\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.data_processing_utils import DatasetMaker\n",
    "\n",
    "ds_maker = DatasetMaker()\n",
    "\n",
    "# Read the dataset\n",
    "\n",
    "df_rhone = gpd.read_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/eval/eval_rhone.gpkg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### code to get trift 2003 Glacier ########################################\n",
    "\n",
    "import dask_geopandas as dg\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "dg_glims = gpd.read_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/eval/glims_polygons.shp\")\n",
    "df_gla = gpd.read_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/eval/TTT.shp\")\n",
    "\n",
    "dg_glims = dg_glims.to_crs('EPSG:3857')\n",
    "\n",
    "glacier_to_check = ['gornergletscher',  'unteraargletscher', 'rhone', 'oberaletsch_gletscher',\n",
    "                    'zmuttgletscher', 'findelengletscher', 'triftgletscher', 'turtmanngletscher',  'allalingletscher',\n",
    "                    'gauligletscher']\n",
    "mask = dg_glims['glac_name'].isin(glacier_to_check)\n",
    "filtered_glims = dg_glims[mask]\n",
    "\n",
    "mask = df_gla['glac_name'].isin(glacier_to_check)\n",
    "\n",
    "filtered_glims['fits'] = filtered_glims['geometry'].apply(lambda x: ds_maker.glacier_fits_bbox_forDataframe(x, 30, 256))\n",
    "\n",
    "glims_fits = filtered_glims[filtered_glims['fits'] == True]\n",
    "\n",
    "#Triftgletscher 2003 seems to be the best fit\n",
    "trift_2003 = glims_fits.loc[glims_fits['glac_name'] == 'triftgletscher']\n",
    "trift_2003 = trift_2003.loc[(trift_2003['glac_id'] == 'G007685E46051N') & (trift_2003['year'] == '2003')]\n",
    "\n",
    "\n",
    "trift_2003.to_file(\"/home/robin/Nextcloud_sn/QGIS/processed/shapefiles/eval/trift_2003.gpkg\", driver='GPKG')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
